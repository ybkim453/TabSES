import os
os.environ["TOKENIZERS_PARALLELISM"] = "false"
import json
import pandas as pd
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import euclidean_distances
import warnings
warnings.filterwarnings('ignore')

outliers=[]

class ColumnSimilarityAnalyzer:
    def __init__(self, model_name='BAAI/bge-large-en'):
        self.model = SentenceTransformer(model_name)
        
    def calculate_column_similarities(self, column_values):
        # ëª¨ë“  ê°’ì„ ë¬¸ìì—´ë¡œ ë³€í™˜ (NaNë„ ê·¸ëŒ€ë¡œ ìœ ì§€)
        str_values = [str(val) for val in column_values]
        
        if len(str_values) < 2:
            return {}
        
        # ì„ë² ë”© ìƒì„±
        embeddings = self.model.encode(str_values)
        
        # ìœ í´ë¦¬ë“œ ê±°ë¦¬ í–‰ë ¬ ê³„ì‚° í›„ ìœ ì‚¬ë„ë¡œ ë³€í™˜
        distance_matrix = euclidean_distances(embeddings)
        # ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜ (ê±°ë¦¬ê°€ ì‘ì„ìˆ˜ë¡ ìœ ì‚¬ë„ê°€ ë†’ìŒ)
        similarity_matrix = 1 / (1 + distance_matrix)
        
        # ê° ê°’ì˜ ìœ ì‚¬ë„ ì´í•© ë° í‰ê·  ê³„ì‚°
        similarities = {}
        for i, val in enumerate(str_values):
            # ìê¸° ìì‹ ì„ ì œì™¸í•œ ë‹¤ë¥¸ ê°’ë“¤ê³¼ì˜ ìœ ì‚¬ë„
            other_similarities = [similarity_matrix[i][j] for j in range(len(str_values)) if i != j]
            
            similarities[i] = {
                'value': val,
                'similarity_sum': sum(other_similarities),
                'similarity_avg': np.mean(other_similarities) if other_similarities else 0,
                'similarity_scores': other_similarities
            }
        
        return similarities
    
    def analyze_table_columns(self, table_df, log_file_path):
        column_analyses = {}
        
        for col in table_df.columns:
            log_message = f"  ì»¬ëŸ¼ '{col}' ë¶„ì„ ì¤‘..."
            print(log_message)
            with open(log_file_path, 'a', encoding='utf-8') as log_f:
                log_f.write(log_message + '\n')
            
            similarities = self.calculate_column_similarities(table_df[col])
            
            if similarities:
                # ìœ ì‚¬ë„ í‰ê· ë“¤ì˜ ë¶„í¬ ê³„ì‚°
                avg_similarities = [sim['similarity_avg'] for sim in similarities.values()]
                
                column_analyses[col] = {
                    'similarities': similarities,
                    'distribution_std': np.std(avg_similarities),
                    'min_similarity': min(avg_similarities),
                    'max_similarity': max(avg_similarities),
                    'mean_similarity': np.mean(avg_similarities)
                }
        
        return column_analyses
    
    def select_rows_for_subtable(self, table_df, column_analyses, log_file_path):
        selected_rows = []
        outliers = []
        
        # íŠ¹ì´ê°’ í–‰ 3ê°œ ì„ íƒ
        extreme_columns = sorted(column_analyses.items(), 
                               key=lambda x: x[1]['distribution_std'], 
                               reverse=True)
        
        anomaly_count = 0
        used_columns = set()
        
        for col_name, col_info in extreme_columns:
            if anomaly_count >= 3:  # Fix: íŠ¹ì´ê°’ í–‰ ê°œìˆ˜ ë³€ê²½ ì‹œ ì—¬ê¸° ìˆ˜ì • (ì˜ˆ: 3ê°œë©´ >= 3)
                break
                
            if col_name in used_columns:
                continue
                
            # ë‚®ì€ ìœ ì‚¬ë„ë¥¼ ê°€ì§„ ê°’ì˜ í–‰ ì°¾ê¸°
            min_sim_row = None
            min_sim_value = float('inf')
            
            for row_idx, sim_info in col_info['similarities'].items():
                if sim_info['similarity_avg'] < min_sim_value:
                    min_sim_value = sim_info['similarity_avg']
                    min_sim_row = row_idx
            
            if min_sim_row is not None and min_sim_row not in selected_rows:
                selected_rows.append(min_sim_row)
                used_columns.add(col_name)
                anomaly_count += 1
                outliers.append(int(min_sim_row))
                log_message = f"  íŠ¹ì´ê°’ í–‰ ì„ íƒ: í–‰ {min_sim_row} (ì»¬ëŸ¼ '{col_name}', ìœ ì‚¬ë„: {min_sim_value:.4f})"
                print(log_message)
                with open(log_file_path, 'a', encoding='utf-8') as log_f:
                    log_f.write(log_message + '\n')
        
        # ì •ìƒê°’ í–‰ 3ê°œ ì„ íƒ
        row_scores = {}
        
        for row_idx in range(len(table_df)):
            if row_idx in selected_rows:
                continue
                
            total_score = 0
            valid_columns = 0
            
            for col_name, col_info in column_analyses.items():
                if row_idx in col_info['similarities']:
                    total_score += col_info['similarities'][row_idx]['similarity_avg']
                    valid_columns += 1
            
            if valid_columns > 0:
                row_scores[row_idx] = total_score / valid_columns
        
        # ê°€ì¥ ë†’ì€ ì ìˆ˜ë¥¼ ê°€ì§„ í–‰ë“¤ ì„ íƒ (ì •í™•íˆ 3ê°œ)
        normal_rows = sorted(row_scores.items(), key=lambda x: x[1], reverse=True)
        normal_count = 0
        
        for row_idx, score in normal_rows:
            if normal_count >= 3:  # Fix: ì •ìƒê°’ í–‰ ê°œìˆ˜ ë³€ê²½ ì‹œ ì—¬ê¸° ìˆ˜ì • (ì˜ˆ: 3ê°œë©´ >= 3)
                break
            if row_idx not in selected_rows:
                selected_rows.append(row_idx)
                log_message = f"  ì •ìƒê°’ í–‰ ì„ íƒ: í–‰ {row_idx} (í‰ê·  ìœ ì‚¬ë„: {score:.4f})"
                print(log_message)
                with open(log_file_path, 'a', encoding='utf-8') as log_f:
                    log_f.write(log_message + '\n')
                normal_count += 1
        
        # ì›ë³¸ í…Œì´ë¸” ìˆœì„œëŒ€ë¡œ ì •ë ¬í•˜ê³  ì •í™•íˆ 6ê°œë§Œ ì„ íƒ
        selected_rows = sorted(list(set(selected_rows)))[:6]  # FIX: í–‰ ì´ ê°œìˆ˜ ë³€ê²½ ì—¬ê¸° ìˆ˜ì • (ì˜ˆ: 3X3ì´ë©° 6ê°œë¡œ [:6])
        log_message = f"  ìµœì¢… ì„ íƒëœ í–‰ (ì›ë³¸ ìˆœì„œ): {selected_rows} (ì´ {len(selected_rows)}ê°œ)"
        print(log_message)
        with open(log_file_path, 'a', encoding='utf-8') as log_f:
            log_f.write(log_message + '\n')
        
        return selected_rows, outliers
    
    # ğŸ”„ ë‘ ë²ˆì§¸ JSON í˜•ì‹ (header + rows)ìœ¼ë¡œ ìˆ˜ì •
    def process_jsonl_record(self, jsonl_path, index, log_file_path):
        log_message = f"\nJSONL íŒŒì¼ì—ì„œ ì¸ë±ìŠ¤ {index} ì²˜ë¦¬ ì¤‘: {jsonl_path}"
        print(log_message)
        with open(log_file_path, 'a', encoding='utf-8') as log_f:
            log_f.write(log_message + '\n')
        
        # JSONL íŒŒì¼ì—ì„œ íŠ¹ì • ì¸ë±ìŠ¤ ë ˆì½”ë“œ ë¡œë“œ
        with open(jsonl_path, 'r', encoding='utf-8') as f:
            for i, line in enumerate(f):
                if i == index:
                    record = json.loads(line.strip())
                    break
            else:
                raise IndexError(f"ì¸ë±ìŠ¤ {index}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # table ë°ì´í„° ì¶”ì¶œ (header + rows í˜•ì‹)
        table = record['table']
        headers = table['header']
        data_rows = table['rows']
        
        log_message = f"  í…Œì´ë¸” í¬ê¸°: {len(data_rows)}í–‰ {len(headers)}ì—´"
        print(log_message)
        with open(log_file_path, 'a', encoding='utf-8') as log_f:
            log_f.write(log_message + '\n')
        
        # DataFrame ìƒì„± (ë¶„ì„ìš©)
        table_df = pd.DataFrame(data_rows, columns=headers)
        
        # ì»¬ëŸ¼ë³„ ìœ ì‚¬ë„ ë¶„ì„
        column_analyses = self.analyze_table_columns(table_df, log_file_path)
        
        if not column_analyses:
            log_message = "  ë¶„ì„ ê°€ëŠ¥í•œ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤."
            print(log_message)
            with open(log_file_path, 'a', encoding='utf-8') as log_f:
                log_f.write(log_message + '\n')
            # ìƒìœ„ 6ê°œ ë°ì´í„° í–‰ ì„ íƒ
            selected_rows = list(range(min(6, len(data_rows))))
            outliers = []
        else:
            # ì„œë¸Œí…Œì´ë¸”ìš© í–‰ ì„ íƒ
            selected_rows, outliers = self.select_rows_for_subtable(table_df, column_analyses, log_file_path)
        
        # ì„ íƒëœ í–‰ë“¤ë¡œ ì„œë¸Œí…Œì´ë¸” êµ¬ì„± (í—¤ë” + ì„ íƒëœ ë°ì´í„°)
        result_lines = ['\t'.join(headers)]  # í—¤ë”
        for i in selected_rows:
            result_lines.append('\t'.join(data_rows[i]))
        
        log_message = f"  ì„œë¸Œí…Œì´ë¸” í¬ê¸°: {len(result_lines)}ì¤„"
        print(log_message)
        with open(log_file_path, 'a', encoding='utf-8') as log_f:
            log_f.write(log_message + '\n')
        
        # âœ… process_single_tableê³¼ ë™ì¼í•˜ê²Œ ë°˜í™˜
        return result_lines, selected_rows, outliers

    def process_single_table(self, file_path, log_file_path):
        log_message = f"\ní…Œì´ë¸” ì²˜ë¦¬ ì¤‘: {file_path}"
        print(log_message)
        with open(log_file_path, 'a', encoding='utf-8') as log_f:
            log_f.write(log_message + '\n')
        
        # ì›ë³¸ íŒŒì¼ì˜ ëª¨ë“  ì¤„ì„ ê·¸ëŒ€ë¡œ ì½ê¸°
        with open(file_path, 'r', encoding='utf-8') as f:
            all_lines = f.readlines()
        
        # í—¤ë”ì™€ êµ¬ë¶„ì„  (ì²« 2ì¤„) - ì›ë³¸ ê·¸ëŒ€ë¡œ ìœ ì§€ (strip í•˜ì§€ ì•ŠìŒ)
        header_line = all_lines[0].rstrip('\n\r')
        separator_line = all_lines[1].rstrip('\n\r')
        
        # ë°ì´í„° ì¤„ë“¤ (3ë²ˆì§¸ ì¤„ë¶€í„°)
        data_lines = [line.strip() for line in all_lines[2:]]
        
        # ë¶„ì„ì„ ìœ„í•´ì„œë§Œ DataFrame ì‚¬ìš© (ì²« ë²ˆì§¸ ì»¬ëŸ¼ì„ ì¸ë±ìŠ¤ë¡œ ì‚¬ìš©)
        temp_df = pd.read_csv(file_path, sep='\t', header=0, index_col=0, na_values=[''], keep_default_na=False)
        data_rows = temp_df.iloc[1:]  # êµ¬ë¶„ì„  ì œì™¸í•œ ë°ì´í„°ë§Œ (í—¤ë”ëŠ” ì´ë¯¸ ì»¬ëŸ¼ëª…)
        
        # ì»¬ëŸ¼ë³„ ìœ ì‚¬ë„ ë¶„ì„
        column_analyses = self.analyze_table_columns(data_rows, log_file_path)
        
        if not column_analyses:
            log_message = "  ë¶„ì„ ê°€ëŠ¥í•œ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤."
            print(log_message)
            with open(log_file_path, 'a', encoding='utf-8') as log_f:
                log_f.write(log_message + '\n')
            # ìƒìœ„ 6ê°œ ë°ì´í„° ì¤„ ì„ íƒ
            selected_data_lines = data_lines[:6]  # FIX: í–‰ ì´ ê°œìˆ˜ ë³€ê²½ ì—¬ê¸° ìˆ˜ì • (ì˜ˆ: 3X3ì´ë©° 6ê°œë¡œ [:6])
        else:
            # ì„œë¸Œí…Œì´ë¸”ìš© í–‰ ì„ íƒ
            selected_rows, outliers = self.select_rows_for_subtable(
            data_rows, column_analyses, log_file_path
        )
            selected_data_lines = [data_lines[i] for i in selected_rows]
        
        # ê²°ê³¼: í—¤ë” + êµ¬ë¶„ì„  + ì„ íƒëœ 6ê°œ ë°ì´í„° ì¤„
        result_lines = [header_line, separator_line] + selected_data_lines
        log_message = f"  ì„œë¸Œí…Œì´ë¸” í¬ê¸°: {len(result_lines)}ì¤„"
        print(log_message)
        with open(log_file_path, 'a', encoding='utf-8') as log_f:
            log_f.write(log_message + '\n')
        
        # âœ… result_lines + ì„ íƒëœ ì¸ë±ìŠ¤ ë°˜í™˜
        return result_lines, selected_rows, outliers


def main():
    # JSONL ì²˜ë¦¬ ì˜ˆì‹œ
    jsonl_path = "datasets/wtq.jsonl"  # âœ… JSONL íŒŒì¼ ê²½ë¡œë¡œ ë³€ê²½
    index = 0  # ì²˜ë¦¬í•  ë ˆì½”ë“œ ì¸ë±ìŠ¤
    output_dir = "subtables"
    
    os.makedirs(output_dir, exist_ok=True)
    analyzer = ColumnSimilarityAnalyzer()
    log_file_path = os.path.join(output_dir, "log.txt")
    
    with open(log_file_path, 'w', encoding='utf-8') as log_f:
        log_f.write("=== JSONL ë ˆì½”ë“œ ì²˜ë¦¬ ì‹œì‘ ===\n")

    try:
        result_lines, selected_rows = analyzer.process_jsonl_record(jsonl_path, index, log_file_path)
        
        # ê²°ê³¼ ì €ì¥
        output_path = os.path.join(output_dir, f"result_{index}.tsv")
        with open(output_path, 'w', encoding='utf-8') as f:
            for line in result_lines:
                f.write(line + '\n')
        
        print(f"âœ… ì €ì¥ ì™„ë£Œ: {output_path}")
        print(f"ì„ íƒëœ í–‰: {selected_rows}")

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")


if __name__ == "__main__":
    main()
